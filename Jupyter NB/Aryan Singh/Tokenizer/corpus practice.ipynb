{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7287f1d0-7f3a-4b84-8386-fc2d6888f231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96cc949-823b-41fd-83c9-fe31dc4b6052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'laxy', 'dog', '.', 'Dogs', 'are', 'great', 'pets', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "corpus = \"The quick brown fox jumps over the laxy dog. Dogs are great pets!\"\n",
    "tokens = word_tokenize(corpus)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c3e906-2e42-4f87-a571-67926530c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'individual', 'words', ',', 'phrases', ',', 'symbols', ',', 'or', 'other', 'meaningful', 'elements', 'called', 'tokens', '.', 'Let', 'me', 'teach', 'you', 'how', 'to', 'perform', 'tokenization', 'using', 'a', 'text', 'corpus', '(', 'a', 'collection', 'of', 'text', 'documents', ')', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "corpus = \"Tokenization is the process of breaking down text into individual words, phrases, symbols, or other meaningful elements called tokens. Let me teach you how to perform tokenization using a text corpus (a collection of text documents).\"\n",
    "tokens = word_tokenize(corpus)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb2af5c-590c-4a75-8661-006de413d9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'individual', 'words', ',', 'phrases', ',', 'symbols', ',', 'or', 'other', 'meaningful', 'elements', 'called', 'tokens', '.', 'Let', 'me', 'teach', 'you', 'how', 'to', 'perform', 'tokenization', 'using', 'a', 'text', 'corpus', '(', 'a', 'collection', 'of', 'text', 'documents', ')', '.']\n",
      "['Tokenization is the process of breaking down text into individual words, phrases, symbols, or other meaningful elements called tokens.', 'Let me teach you how to perform tokenization using a text corpus (a collection of text documents).']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "corpus = \"Tokenization is the process of breaking down text into individual words, phrases, symbols, or other meaningful elements called tokens. Let me teach you how to perform tokenization using a text corpus (a collection of text documents).\"\n",
    "tokens = word_tokenize(corpus)\n",
    "print(tokens)\n",
    "tokens = sent_tokenize(corpus)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa218181-31a8-4920-8b57-31ab565b7ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is the process of breaking down text into individual words, phrases, symbols, or other meaningful elements called tokens.', 'Let me teach you how to perform tokenization using a text corpus (a collection of text documents).']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(corpus)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e905abca-0c06-4d9c-a531-00c0fb262426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr', '.', 'Singh', ',', 'how', 'are', 'you', '?', 'It', \"'s\", '$12.32.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d.]+|\\S+')\n",
    "tokens = tokenizer.tokenize(\"Hello Mr. Singh, how are you? It's $12.32.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67369d06-a0a4-4214-97f5-cb3e6d7b5e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4634d-0be2-44e9-a117-fb4e7066adda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f26b53-bf67-45ca-b2f1-e2e965a334c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c1ee3-4c52-4339-a725-37c06c4e2d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
